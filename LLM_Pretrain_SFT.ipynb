{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Pretrain"
   ],
   "metadata": {
    "id": "GNyPw9hYiGdF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Souzy9izJnsw",
    "outputId": "4e9248ea-65df-446b-9034-6cffa87484d1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'RussianNovels'...\n",
      "remote: Enumerating objects: 119, done.\u001b[K\n",
      "remote: Total 119 (delta 0), reused 0 (delta 0), pack-reused 119 (from 1)\u001b[K\n",
      "Receiving objects: 100% (119/119), 21.67 MiB | 13.76 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JoannaBy/RussianNovels.git"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "repo_path = 'RussianNovels'\n",
    "text_files_pattern = os.path.join(repo_path, '**', '*.txt')\n",
    "\n",
    "# Search for all .txt files in all subdirectories\n",
    "all_files = glob.glob(text_files_pattern, recursive=True)\n",
    "\n",
    "# Merge the content of all files into one large string\n",
    "full_corpus = \"\"\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            full_corpus += f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Print corpus size information and save it\n",
    "print(f\"Total files found: {len(all_files)}\")\n",
    "print(f\"Total corpus size (in characters): {len(full_corpus)}\")\n",
    "\n",
    "# Save the merged corpus to a single file\n",
    "output_filename = 'russian_novels_corpus.txt'\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(full_corpus)\n",
    "\n",
    "print(f\"Corpus saved to file: {output_filename}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkqkAq9DOhze",
    "outputId": "e1743b60-8e45-49df-ed23-9316565ed69b"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files found: 108\n",
      "Total corpus size (in characters): 45348575\n",
      "Corpus saved to file: russian_novels_corpus.txt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "with open('russian_novels_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "     full_corpus = f.read()\n",
    "\n",
    "# Split text into lines (assuming each line/paragraph is a potential unit)\n",
    "lines = full_corpus.split('\\n')\n",
    "lines = [line.strip() for line in lines if line.strip()]  # Remove empty lines\n",
    "\n",
    "# Deduplication: Removes completely identical lines/paragraphs\n",
    "def remove_duplicates(data_list):\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "    for item in data_list:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            unique_data.append(item)\n",
    "    return unique_data\n",
    "\n",
    "lines_unique = remove_duplicates(lines)\n",
    "print(f\"Lines after deduplication: {len(lines_unique)} (Removed: {len(lines) - len(lines_unique)})\")\n",
    "\n",
    "# Non-Cyrillic filtering: Removes lines containing Latin characters\n",
    "LATIN_PATTERN = re.compile(r'[a-zA-Z]')\n",
    "\n",
    "def filter_non_cyrillic(data_list):\n",
    "    filtered_data = [line for line in data_list if not LATIN_PATTERN.search(line)]\n",
    "    return filtered_data\n",
    "\n",
    "lines_cyrillic = filter_non_cyrillic(lines_unique)\n",
    "print(f\"Lines after non-Cyrillic filtering: {len(lines_cyrillic)} (Removed: {len(lines_unique) - len(lines_cyrillic)})\")\n",
    "\n",
    "# Handling repeating punctuation and normalization\n",
    "def normalize_punctuation(text):\n",
    "    text = re.sub(r'\\.{2,}', '...', text) # replace 2 or more dots with '...'\n",
    "    text = re.sub(r'([?!])\\1+', r'\\1', text) # '!!!' -> '!', '???' -> '?'\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s([,.!?:;])', r'\\1', text) # remove extra spaces before punctuation\n",
    "\n",
    "    return text\n",
    "\n",
    "lines_normalized = [normalize_punctuation(line) for line in lines_cyrillic]\n",
    "\n",
    "# Merging cleaned and normalized lines back into a single corpus\n",
    "cleaned_corpus = '\\n'.join(lines_normalized)\n",
    "\n",
    "print(f\"Final corpus size (characters): {len(cleaned_corpus)}\")\n",
    "print(\"Example of cleaned text (first 500 chars):\")\n",
    "print(cleaned_corpus[:500])\n",
    "\n",
    "# Saving cleaned corpus for the next step\n",
    "output_filename_cleaned = 'russian_novels_corpus_cleaned.txt'\n",
    "with open(output_filename_cleaned, 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_corpus)\n",
    "\n",
    "print(f\"\\nCleaned corpus saved to file: {output_filename_cleaned}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pUQjcOJRUCH",
    "outputId": "e6008c33-8ca4-4e82-a8c6-25ce192d7df0"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lines after deduplication: 304839 (Removed: 10652)\n",
      "Lines after non-Cyrillic filtering: 295808 (Removed: 9031)\n",
      "Final corpus size (characters): 39657833\n",
      "Example of cleaned text (first 500 chars):\n",
      "В начале 1928 года в Берлине знатоку живописи Бруно Кречмару, человеку, очень, кажется, сведущему, но отнюдь не блестящему, пришлось быть экспертом в пустячном, прямо даже глупом деле. Модный художник Кок написал портрет фильмовой артистки Дорианны Карениной. Фирма личных кремов приобретала у нее право помещать на плакатах репродукцию с портрета в виде рекламы своей губной помады. На портрете Дорианна держала прижатой к голому своему плечу большую плюшевую Чипи. Горн из Нью-Йорка тотчас предъяви\n",
      "\n",
      "Cleaned corpus saved to file: russian_novels_corpus_cleaned.txt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8a6fbc5",
    "outputId": "454c5d09-b144-488d-c6a6-93aeccb761b0"
   },
   "source": [
    "# Tokenizer Setup\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors, decoders\n",
    "\n",
    "# Define the tokenizer architecture (BPE)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Configure the pre-tokenizer to split input using ByteLevel encoding\n",
    "# This handles raw bytes, allowing the model to process any unicode string\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "# Define vocabulary size and special tokens\n",
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "# Train the tokenizer on the cleaned corpus\n",
    "tokenizer.train([\"russian_novels_corpus_cleaned.txt\"], trainer=trainer)\n",
    "\n",
    "# Post-Processor setup: automatically adds <bos> and <eos> tokens to sequences\n",
    "# Logic: <bos> + Sequence + <eos>\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"<bos> $A <eos>\",\n",
    "    pair=f\"<bos> $A <eos> <eos> $B <eos>\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "        (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure the decoder to match the ByteLevel pre-tokenizer\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Testing the Tokenizer\n",
    "test_text = \"Давайте проверим этот токенайзер.\"\n",
    "encoding = tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Tokens (encoding.tokens): {encoding.tokens}\")\n",
    "\n",
    "# Verify decoding\n",
    "decoded_text = tokenizer.decode(encoding.ids)\n",
    "print(f\"Decoded text: {decoded_text}\")\n",
    "\n",
    "tokenizer.save(\"bpe_russian_novels_tokenizer.json\")"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original text: Давайте проверим этот токенайзер.\n",
      "Tokens (encoding.tokens): ['<bos>', 'ÐĶ', 'Ð°Ð²', 'Ð°Ð¹ÑĤÐµ', 'ĠÐ¿ÑĢÐ¾Ð²ÐµÑĢ', 'Ð¸Ð¼', 'ĠÑįÑĤÐ¾ÑĤ', 'ĠÑĤÐ¾Ðº', 'ÐµÐ½', 'Ð°Ð¹', 'Ð·', 'ÐµÑĢ', '.', '<eos>']\n",
      "Decoded text: Давайте проверим этот токенайзер.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONTEXT_LENGTH = 512  # Context window size for the model\n",
    "TOKENIZER_FILE = \"bpe_russian_novels_tokenizer.json\"\n",
    "DATA_FILE = \"russian_novels_corpus_cleaned.txt\"\n",
    "\n",
    "# Load the pre-trained tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_FILE)\n",
    "\n",
    "# Define special tokens established during the training phase\n",
    "tokenizer.bos_token = \"<bos>\"\n",
    "tokenizer.eos_token = \"<eos>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "tokenizer.mask_token = \"<mask>\"\n",
    "\n",
    "# Set pad_token_id explicitly to prevent training errors\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "# Load data into a Hugging Face Dataset object\n",
    "# 'load_dataset' creates a dataset with a single column named 'text'\n",
    "raw_datasets = load_dataset('text', data_files={'train': DATA_FILE})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenizes the text without truncation.\"\"\"\n",
    "    # truncation=False ensures we keep full sequences to group them later into blocks\n",
    "    return tokenizer(examples[\"text\"], truncation=False)\n",
    "\n",
    "# Apply tokenization to the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,                # Process in batches for speed\n",
    "    num_proc=os.cpu_count(),     # Use all available CPU cores\n",
    "    remove_columns=[\"text\"],     # Remove the raw text column to save memory\n",
    ")\n",
    "\n",
    "# Chunking logic: Groups texts into blocks of size 512\n",
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    Concatenates all texts and splits them into chunks of CONTEXT_LENGTH.\n",
    "    This prepares the data for Language Modeling (CLM/MLM).\n",
    "    \"\"\"\n",
    "    concatenated_examples = {}\n",
    "\n",
    "    # Concatenate all 'input_ids' into a single list\n",
    "    for key, list_of_lists in examples.items():\n",
    "        concatenated_examples[key] = sum(list_of_lists, [])\n",
    "\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Truncate the total length to be divisible by CONTEXT_LENGTH\n",
    "    total_length = (total_length // CONTEXT_LENGTH) * CONTEXT_LENGTH\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Split into chunks\n",
    "    for k, t in concatenated_examples.items():\n",
    "        chunks_list = []\n",
    "        for i in range(0, total_length, CONTEXT_LENGTH): # Step size: 512 tokens\n",
    "            chunk = t[i : i + CONTEXT_LENGTH]\n",
    "            chunks_list.append(chunk)\n",
    "        result[k] = chunks_list\n",
    "\n",
    "    # Add 'labels' column: for Language Modeling, labels are usually identical to input_ids\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Apply the chunking transformation\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    ")\n",
    "\n",
    "final_train_dataset = lm_datasets[\"train\"]\n",
    "\n",
    "print(f\"Final number of training chunks: {len(final_train_dataset)}\")\n",
    "print(f\"Length of a single chunk: {len(final_train_dataset[0]['input_ids'])} tokens\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Example of the first chunk (input_ids):\")\n",
    "print(final_train_dataset[0][\"input_ids\"])\n",
    "print(\"Decoded example of the first chunk:\")\n",
    "print(tokenizer.decode(final_train_dataset[0][\"input_ids\"]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255,
     "referenced_widgets": [
      "fb0031c7964a49d596c1bc3207138a07",
      "7c5edab6fbff4eaf8ebc5236733b22e1",
      "6438fc28ffe54ce189593f660a10a59e",
      "b7bc093879614092a41194fb042f64ca",
      "0be21bb79ef34ebc9cdec3f11ff6df11",
      "11bd8ca67e6f4e8eb785df707b47c3bd",
      "cc1c146524a247f0b7baad653b36ae5d",
      "1562d60e49fa4737933301319c38ed83",
      "abab75160ea542119972c569af01578b",
      "145d1dc4e76e4497b2f3542e7e25e513",
      "d01b845bb8914b74b0af810bbd16cb57",
      "cf5c954f95d1430f9a3531c249c3e7b4",
      "4fa021b48bbd471da3fc69d1bd8d7f21",
      "f0ac5f2c84b54968b950f5306a53ea2f",
      "c41e72deaf0849b38dc3d5ff9dc0971e",
      "dcd8858f1e38421e8e609735dce7f881",
      "2fe84accfc7b471dafa520602a8c7568",
      "458600d92a944a13b1c55b1e38ef2fc2",
      "c6fc36723e9e4829bbf38dcf51c467ca",
      "ef5e6e33669a42f58fea7db571274897",
      "87d13c0ed52149008ff295a64042cb8e",
      "8855263b1f15437ea3e3f8c866601e24",
      "e70fc7544d8b41dd9d594a661f2c102c",
      "9b4c0513a44a490d8f9f0cae96560b5b",
      "bb36e5379d6741ac82466a8b0705847f",
      "5dac15d147744c1f9e752e3810ad2a19",
      "9b812a0a63bc44ecb83521e7eac20a74",
      "7dda31414db0404d83b0c36220d72bc8",
      "5c007a1e8cf64d98b4a9a5717e50ecc4",
      "65ea5a0bc5234d608d348bba914a24c6",
      "d70c572b32024c28a47fda92dc4a765d",
      "3997b6249690400896e89f7e835a6b6f",
      "c22cabac4af943649f291e7ca1690c84"
     ]
    },
    "id": "7O1Pjs5EJGiP",
    "outputId": "c67bf01b-a4fc-41f7-a07f-51f2b5e2e03a"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb0031c7964a49d596c1bc3207138a07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/295808 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf5c954f95d1430f9a3531c249c3e7b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/295808 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e70fc7544d8b41dd9d594a661f2c102c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final number of training chunks: 19996\n",
      "Length of a single chunk: 512 tokens\n",
      "------------------------------------------------------------\n",
      "Example of the first chunk (input_ids):\n",
      "[0, 507, 6632, 14420, 27, 2023, 140, 22909, 6495, 5424, 9021, 124, 22697, 165, 5494, 130, 15, 3734, 15, 582, 15, 1472, 15, 6120, 848, 443, 15, 342, 15019, 177, 3838, 443, 15, 4446, 571, 10980, 992, 164, 140, 1498, 2716, 555, 15, 1598, 611, 1518, 164, 1955, 17, 303, 176, 322, 13594, 23294, 5596, 6090, 16093, 146, 1429, 10922, 275, 7578, 124, 208, 196, 2375, 154, 1270, 17, 599, 388, 867, 2565, 415, 14914, 151, 9499, 267, 175, 836, 2767, 3189, 226, 185, 2615, 217, 387, 8031, 491, 730, 3484, 136, 21640, 140, 3387, 19653, 135, 917, 1078, 332, 578, 5244, 17, 946, 4274, 558, 7578, 12573, 9458, 2584, 5635, 149, 392, 363, 2659, 8190, 5264, 448, 1488, 22657, 470, 1131, 124, 17, 11304, 292, 17806, 16, 12287, 183, 276, 1294, 19562, 166, 20715, 1254, 1017, 17, 1, 0, 2090, 478, 1449, 404, 638, 2486, 4376, 1, 0, 14718, 361, 14298, 866, 5705, 9556, 4319, 144, 21644, 15, 177, 301, 140, 361, 8512, 12874, 6432, 15, 588, 1082, 3157, 3174, 4103, 702, 1769, 15, 620, 18944, 984, 17, 14759, 15, 13747, 8138, 322, 1160, 15, 23232, 15, 441, 185, 1906, 2687, 980, 15, 8156, 474, 15, 211, 1609, 1646, 1624, 1523, 17, 6410, 1209, 6955, 166, 1231, 15083, 15, 145, 5946, 1619, 329, 271, 177, 878, 353, 15, 264, 1860, 5993, 1202, 145, 1926, 353, 15, 145, 238, 15, 177, 7149, 15, 1579, 1296, 136, 16436, 8212, 2404, 185, 8829, 15, 145, 18637, 4736, 6226, 13256, 254, 2662, 1074, 2315, 3218, 1646, 185, 1387, 1160, 17, 563, 593, 15, 140, 813, 1656, 771, 15, 140, 813, 2905, 737, 709, 771, 15, 5494, 15, 10189, 6781, 221, 18039, 15, 2525, 415, 5590, 216, 3516, 15, 592, 474, 4523, 1897, 5819, 271, 15, 5630, 187, 678, 6962, 393, 20316, 17, 422, 2877, 412, 15, 136, 15728, 15, 211, 346, 15, 9332, 570, 1347, 254, 2715, 238, 15, 264, 2662, 24471, 1913, 145, 689, 29, 422, 6202, 576, 15, 243, 474, 9679, 756, 422, 2090, 290, 1395, 15, 1748, 5567, 15, 177, 290, 1395, 856, 254, 824, 349, 6609, 329, 149, 1728, 471, 15, 12562, 3727, 652, 11744, 1254, 15, 1482, 883, 9336, 953, 561, 3484, 19724, 365, 328, 11958, 17, 422, 606, 9390, 10565, 20300, 3870, 2006, 254, 1344, 6565, 15, 15503, 1852, 9035, 179, 17, 5494, 1512, 29, 422, 24827, 429, 949, 528, 738, 3003, 180, 15, 528, 5082, 1212, 1896, 1933, 15, 528, 19724, 365, 328, 11958, 253, 5113, 1891, 9674, 3501, 12185, 431, 17, 3042, 291, 247, 19738, 286, 15, 829, 3585, 568, 140, 948, 2006, 1, 0, 8717, 306, 145, 15224, 20382, 254, 1392, 20382, 860, 15, 211, 5494, 140, 7156, 17467, 1010, 157, 1606, 332, 1110, 177, 16687, 5149, 486, 6508, 15, 240, 3369, 2892, 155, 222, 1085, 486, 6508, 177, 16687, 17, 422, 18938, 1085, 2745, 15, 254, 1512, 238, 15, 254, 6125, 209, 17457, 381, 122, 311, 994, 15, 600, 729, 177, 994, 15, 342, 7981, 136, 723, 185, 741, 292, 3671, 4921, 15, 600, 6979, 149, 12510, 17793, 381, 3023, 15, 600, 1286, 236, 16]\n",
      "Decoded example of the first chunk:\n",
      "<bos>В начале 1928 года в Берлине знатоку живописи Бруно Кречмару, человеку, очень, кажется, сведущему, но отнюдь не блестящему, пришлось быть экспертом в пустячном, прямо даже глупом деле. Модный художник Кок написал портрет фильмовой артистки Дорианны Карениной. Фирма личных кремов приобретала у нее право помещать на плакатах репродукцию с портрета в виде рекламы своей губной помады. На портрете Дорианна держала прижатой к голому своему плечу большую плюшевую Чипи. Горн из Нью-Йорка тотчас предъявил фирме иск.<eos><bos>Не тронь меня (лат.).<eos><bos>Эта ее привычка задавать зря вопросы о предметах, не раз в ее присутствии обсуждавшихся, была следствием скорее нервности мысли, чем невнимания. Часто, задав рассеянный вопрос, Аннелиза, еще на разгоне слова, понимала уже, что давно сама знает ответ. Муж хорошо изучил эту привычку, и нисколько прежде она его не сердила, а лишь умиляла и смешила, и он, не отвечая, продолжал разговор с выжидательной улыбкой на губах, и ожидание обыкновенно оправдывалось - жена почти сразу отвечала сама на свой вопрос. Но теперь, в этот именно день, в этот мартовский день, Кречмар, трепещущий от странных, тайных переживаний, вот уже неделю мучивших его, проникся вдруг необычайным раздражением. \"Что ты, с луны, что ли, свалилась?\" - воскликнул он, а жена махнула рукой и сказала: \"Ах да, я уже вспомнила\". \"Не так быстро, мое дитя, не так быстро\", - тут же обратилась она к дочке, восьмилетней Ирме, которая пожирала свою порцию шоколадного крема. \"С точки зрения юридической...\" - начал Макс, пыхтя сигарой. Кречмар подумал: \"Какое мне дело до этого Горна, до рассуждений Макса, до шоколадного крема... Со мной происходит нечто невероятное. Надо затормозить, надо взять себя в руки...\"<eos><bos>Было это и впрямь невероятно - особенно невероятно потому, что Кречмар в течение девяти лет брачной жизни не изменил жене ни разу, по крайней мере действенно ни разу не изменил. \"Собственно говоря, - подумал он, - следовало бы Аннелизе все сказать, или ничего не сказать, но уехать с ней на время из Берлина, или пойти к гипнотизеру, или наконец как-\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "\n",
    "configuration = LlamaConfig(hidden_size=1024, intermediate_size=1536, num_hidden_layers=16, num_attention_heads=16, num_key_value_heads=8, vocab_size=25000)\n",
    "model = LlamaForCausalLM(configuration)\n",
    "model.to(device)\n",
    "configuration = model.config\n",
    "\n",
    "total_params = model.num_parameters()\n",
    "print(f\"Number of model's parameters: {total_params / 1_000_000:.2f}M\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMJzTHnPR8rP",
    "outputId": "42ec82fb-42eb-4615-a221-e8716a67b27e"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of model's parameters: 177.06M\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer, PreTrainedTokenizerFast\n",
    "\n",
    "CONTEXT_LENGTH = 512\n",
    "VOCAB_SIZE = 25000\n",
    "\n",
    "split_datasets = lm_datasets['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "\n",
    "    eval_strategy=\"steps\", # Estimation on a validation set\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=100,\n",
    "\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "metadata": {
    "id": "Wi67MaMpWr-o"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "]"
   ],
   "metadata": {
    "id": "JvQxCtlPdcCP"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "import torch\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to evaluate model generation quality during training.\n",
    "    It generates text based on a list of test prompts at the end of each evaluation phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_prompts, tokenizer, model):\n",
    "        self.test_prompts = test_prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "\n",
    "        # Switch model to evaluation mode (disables dropout, etc.)\n",
    "        self.model.eval()\n",
    "        device = self.model.device\n",
    "\n",
    "        print(f\"\\n--- Generating samples at step {state.global_step} ---\")\n",
    "\n",
    "        for prompt in self.test_prompts:\n",
    "            # Tokenize the prompt and move to the active device\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_length=inputs.shape[1] + 150, # Generate up to 150 *new* tokens\n",
    "                    do_sample=True,    # Enable sampling strategy (probabilistic generation)\n",
    "                    temperature=0.7,   # Control randomness: lower = more deterministic, higher = more creative\n",
    "                    top_k=50,          # Top-K sampling: limit selection to the top 50 probability tokens\n",
    "                )\n",
    "\n",
    "            # Decode the generated tokens back to text\n",
    "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "            print(f\"Generated: {generated_text}\\n\")\n",
    "\n",
    "        # Switch model back to training mode to resume training correctly\n",
    "        self.model.train()"
   ],
   "metadata": {
    "id": "zsrqZP6BdkOX"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generation_callback = GenerationCallback(test_prompts, tokenizer, model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[generation_callback]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OAQYMEZ1d8YI",
    "outputId": "98bae65a-7cdd-4082-a611-f92094dddcc0"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-1008094080.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 0, 'pad_token_id': 2}.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 1:31:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.562600</td>\n",
       "      <td>6.886019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.597500</td>\n",
       "      <td>6.351955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.231700</td>\n",
       "      <td>6.097020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.004000</td>\n",
       "      <td>5.932364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.872200</td>\n",
       "      <td>5.819438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.767800</td>\n",
       "      <td>5.741858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.691600</td>\n",
       "      <td>5.691276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5.650300</td>\n",
       "      <td>5.663435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Generating samples at step 100 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствияПс,\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духаВим; и, что на-то!\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страдания-- Да, что я не она, и что я в она, и не ты.\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободным- Ну, и как на в ее и, я в этом с\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будуВ-нибудь.\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смертиВ в поать в, а вы, что и, и, у,\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не кончена- К-то, что, не вы на меня.\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простая- К не мне, и в ее, но.\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое дело\"ки, не не от бы и в еще неал. Но, что у\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честно-- Я, с меня, -- и не сказала на него и не вдруг и еще, -- а, так и это, что что, не быть, что за ней. Я не только не теперь, а только когда меня, что по меня, и, -- я не только он, -- -- как я -- и все, -- что.\n",
      "\n",
      "\n",
      "--- Generating samples at step 200 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствиявобдо-то ему, что он он у вас, что он\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духаНо-то-то-то, не сказал он, все это, что он не\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страданияпотин.\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободнымВи за-с и и все, и он не это, что он он в\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будуи, что он было было, что он, но все что,\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смерти-- Да, -- сказал он не было, а я не могу, -- я\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не конченатаке, я еще не то, в руках, что он все жая на\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простаявИ и с меня было, что он не ни с ним и\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое дело-- Нет, что?\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честно-- И не не! И в и он, что в что же же от меня еще не\n",
      "\n",
      "\n",
      "--- Generating samples at step 300 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствияпо- За вами.\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духаС-моему, где ж-то! - не так что, как они, и\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страданияпго-то не только, что он не мог. Ведь он так, что я не\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободнымМгли отный. Но, как он, не был в его, и\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будусару, а не то, что ты?\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смерти- Но, - чтобы я был и не совсем.\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не конченапорази, и что он не помню, -- сказал Пьер, что ее\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простаясдарны, не сказал:\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое делотобут, он не думал, как и не могу уже.\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честно-- Да, что вы, и вам это, и что я теперь все-нибудь.\n",
      "\n",
      "\n",
      "--- Generating samples at step 400 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствиякА и к меня не могу!\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духаотрных.\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страданияпридели в том, что он не в чем он так, как в\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободнымНо у нас за лестнице, что я не мог не успел.\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будузнул, что за все-таки в этот раз же минуту? А не\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смерти-- Наай за себя, не знаете, -- сказал он. -- Я ведь не знал, что вы\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не конченанешоблади и не сказал -- с тобой.\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простаяпосорнул.\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое делокак-нибудь. И не может быть, что он уже с удивлением и все уже\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честно\"Дота не не будет мне, а он и, как будто\n",
      "\n",
      "\n",
      "--- Generating samples at step 500 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствия-- Да, я тебя люблю, -- возразил Николай.\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духасок, которые он был бы сделать, и он, не только он очень\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страданияСуратных и в дом. У него, в котором он был в сторону\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободнымнаы, но не в доме его не могу.\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будусобрал и что этот раз говорить о том что, что это я не\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смертиВажанных?\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не конченакак раз, когда сам уже не стал. Он по-русски стал\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простаяподсра. Он не мог быть ни одного, ни на все время,\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое делоссра, -- сказал Самгин, когда он не только что не буду\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честнокудить.\n",
      "\n",
      "\n",
      "--- Generating samples at step 600 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствия-- Как же, -- сказал Фома, когда я знал как-то и не то, что он\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духав-у.\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страданиявыверта, и в Петербурге, он был говорил с кем-то\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободнымпавской.\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будузать: я не мог...\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смертирасбру, а то, что в нем было я не было, а мы не\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не конченаот-с, если бы все-таки не только не только что-то говорить.\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простаябела к ним, в котором он не в таком случае и не то, как и\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое делопестил на него, когда они, как он, без того не было.\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честнопраиз-то, что не зная, что в этом-то и в нем в\n",
      "\n",
      "\n",
      "--- Generating samples at step 700 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствияпо-подсину, - не в этом дело!\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духаВобврий и, что я теперь мог говорить, что я сам не\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страданиянена-м-с, и там, что я, ну, я\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободным- Не не вы, ты, - сказал он, - она в первый раз\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будувразмело, что я его не мог этого. И так, чтобы вы не\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смертиА-то, а в том, что же вы и не так, что вы\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не конченапо-кто-нибудь.\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простаякещала в жизни, - и он думал.\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое деловездно-то - все-таки, что мы, за ним, не\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честнокворы, что за что-то не мог, а?\n",
      "\n",
      "\n",
      "--- Generating samples at step 800 ---\n",
      "Prompt: \"Все мысли, которые имеют огромные последствия\"\n",
      "Generated: Все мысли, которые имеют огромные последствиябеди, чем они в ее месте.\n",
      "\n",
      "Prompt: \"Сила войска зависит от его духа\"\n",
      "Generated: Сила войска зависит от его духаднау и не не могла.\n",
      "\n",
      "Prompt: \"Мысль о том, что он принес страдания\"\n",
      "Generated: Мысль о том, что он принес страданиямоденила. На другой день он, как будто на него,\n",
      "\n",
      "Prompt: \"Человек сознает себя свободным\"\n",
      "Generated: Человек сознает себя свободнымбешив, и так как и как бы он сам не знал.\n",
      "\n",
      "Prompt: \"Что бы ни случилось, я всегда буду\"\n",
      "Generated: Что бы ни случилось, я всегда будупегали, что мне все-таки они, я все-таки не могу.\n",
      "\n",
      "Prompt: \"Любовь мешает смерти\"\n",
      "Generated: Любовь мешает смерти- А ты не сказал? - спросил он, что я - с\n",
      "\n",
      "Prompt: \"Нет, жизнь не кончена\"\n",
      "Generated: Нет, жизнь не конченакдудите.\n",
      "\n",
      "Prompt: \"Всякая мысль, даже самая простая\"\n",
      "Generated: Всякая мысль, даже самая простаяотстрина, не в этом, на котором он, не желая дело, но не\n",
      "\n",
      "Prompt: \"Война не любезность, а самое гадкое дело\"\n",
      "Generated: Война не любезность, а самое гадкое делобляко и не мог было так же жить, что он не знала, что\n",
      "\n",
      "Prompt: \"Чтобы жить честно\"\n",
      "Generated: Чтобы жить честновпим-с.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=846, training_loss=6.142734753042812, metrics={'train_runtime': 5508.2999, 'train_samples_per_second': 9.801, 'train_steps_per_second': 0.154, 'total_flos': 2.512029601706803e+16, 'train_loss': 6.142734753042812, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Post-train SFT"
   ],
   "metadata": {
    "id": "RFzXtu5kiMuX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "raw_dataset = load_dataset(\"d0rj/alpaca-cleaned-ru\", split=\"train\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "dc6b20906c3a4fd5870d93d549654066",
      "411dc50a73c04c4ba63ed93955b890d0",
      "1004e21705594967be8a2459ee7be35a",
      "90cfa77e5da54dbd9fcbcff9a59a457b",
      "c4f2331403b04c30bf73d7f3b8c3dd81",
      "9f31ea0078f5421db254afcbafd5adad",
      "57ba968bdec340f38047366a7bdfe710",
      "59a5be3bd1424723981a06fe06aa6d46",
      "0ccd6ea93cfe4442b14aff112a720a32",
      "f58443b717024cd6a2fa2a1377896dce",
      "32c5ff7714da45019fa09833f8df48f7",
      "f4edc915dca4441eb4be5376e70e8b1a",
      "2171c06848174118b021ead27a883d04",
      "8956f54027d54213a50c224c99dd78fb",
      "c549ef34bd7d432dad199193169f9149",
      "78e67dfb679940bc9cf95bc375eb9ab2",
      "d49f7ae5f64b4798a245b4a89ca63345",
      "8b299ced05f44f9585ffb753cf4f3f8f",
      "143e8dce7c3240399116bdd47625fe32",
      "9cf2b2dfa13f455087a4eeb7304cc6e4",
      "de0777710ee2453f9a6f3f2c48128270",
      "2cf8cda3f938434e81a770dbc6744359",
      "55703b62c5ae40e6bb10d57e95aeaa9f",
      "8c4fc1f4930144f2af000d7817dcb1fe",
      "85df0b6fb65247deb90d494435070520",
      "361a21483a4f41c0bb7937a245c5e81a",
      "03b8ccecb28e4c9e95bc52f3c1c7d63b",
      "406e1cc142d54c6fabbd54525d92513c",
      "257a72f90822475f950ece1485a3cdbc",
      "c5f5e656eef646a590591cc1024325df",
      "d2724d72a90549c6a99885bc7ff6a388",
      "b2a182cff2de45d490728cb2d01fe1fd",
      "9f36b6336d274df2b5bf491d57f9033a"
     ]
    },
    "id": "KFecJyEjiPXL",
    "outputId": "06e3edbd-3d3d-4b68-c427-7e7eebed3584"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc6b20906c3a4fd5870d93d549654066"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001-c503683bee003a(…):   0%|          | 0.00/36.6M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4edc915dca4441eb4be5376e70e8b1a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55703b62c5ae40e6bb10d57e95aeaa9f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(raw_dataset[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxAGnkMlyrjn",
    "outputId": "e22176f0-f07a-414a-efda-89ee5d54f631"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input': '', 'instruction': 'Дайте три совета, как оставаться здоровым.', 'output': '1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\\n\\n2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее значение для поддержания крепких костей, мышц и здоровья сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут умеренным аэробным упражнениям или 75 минут интенсивным упражнениям каждую неделю.\\n\\n3. Высыпайтесь. Достаточное количество качественного сна имеет решающее значение для физического и психического благополучия. Он помогает регулировать настроение, улучшать когнитивные функции и поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 часов каждую ночь.'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def format_alpaca_to_dialogue(example):\n",
    "    \"\"\"\n",
    "    Transforms an Alpaca-style example (instruction, input, output) into a single text string.\n",
    "    \"\"\"\n",
    "    system = \"\"\n",
    "    if example[\"input\"]:\n",
    "        system = example['input'] + \"\\n\"\n",
    "\n",
    "    user = example['instruction']\n",
    "    assistant = example['output']\n",
    "\n",
    "    # Combine fields into a single sequence\n",
    "    full_text = system + user + \"\\n\\n\" + assistant\n",
    "\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "# Apply formatting to the entire dataset\n",
    "formatted_dataset = raw_dataset.map(\n",
    "    format_alpaca_to_dialogue,\n",
    "    num_proc=os.cpu_count(),\n",
    "    # Drop original columns (instruction, input, output) to keep only the processed 'text'\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Total number of examples: {len(formatted_dataset)}\")\n",
    "print(\"Sample of the first formatted dialogue:\")\n",
    "print(formatted_dataset[0][\"text\"])\n",
    "\n",
    "split_datasets = formatted_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]\n",
    "\n",
    "print(f\"\\nDataset Split: Train={len(train_dataset)}, Validation={len(eval_dataset)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "9a7ed47a8def4ac786a1583bf2b14c3a",
      "582937af44d24d948c3c1ca4c49eecde",
      "43a9f2de76c14416a9902413b5617503",
      "dd48c093d510426c96fec1b951ae4a99",
      "70349538bfaf4d2896081e519286f572",
      "f569364d31a2483cb946d9903990a531",
      "b04e97d9e49e4463933384c446bd3a68",
      "4a6d3f53cdf049f395147c64c3263503",
      "9506b73b7cae4c68b316399e083937d2",
      "d9ac2a51bb434d95889168a639533663",
      "1e5373cf76dc46f5a545d8524204409f"
     ]
    },
    "id": "cnipkMXsylJA",
    "outputId": "8aacfeee-96e2-4697-af46-62f3e920d7fc"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a7ed47a8def4ac786a1583bf2b14c3a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of examples: 51760\n",
      "Sample of the first formatted dialogue:\n",
      "Дайте три совета, как оставаться здоровым.\n",
      "\n",
      "1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\n",
      "\n",
      "2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее значение для поддержания крепких костей, мышц и здоровья сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут умеренным аэробным упражнениям или 75 минут интенсивным упражнениям каждую неделю.\n",
      "\n",
      "3. Высыпайтесь. Достаточное количество качественного сна имеет решающее значение для физического и психического благополучия. Он помогает регулировать настроение, улучшать когнитивные функции и поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 часов каждую ночь.\n",
      "\n",
      "Dataset Split: Train=49172, Validation=2588\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(formatted_dataset[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcKgex7o1gN_",
    "outputId": "72633af8-c85b-4ace-8044-b59be36e9aaf"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': 'Дайте три совета, как оставаться здоровым.\\n\\n1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\\n\\n2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее значение для поддержания крепких костей, мышц и здоровья сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут умеренным аэробным упражнениям или 75 минут интенсивным упражнениям каждую неделю.\\n\\n3. Высыпайтесь. Достаточное количество качественного сна имеет решающее значение для физического и психического благополучия. Он помогает регулировать настроение, улучшать когнитивные функции и поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 часов каждую ночь.'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pip install trl"
   ],
   "metadata": {
    "id": "88GfoIkv6BfB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # Optimize memory usage with BF16 or FP16 precision\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    # Automatically map model layers to available devices (GPU/CPU sharding)\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set PAD token to EOS token (Crucial for SFT to ensure correct masking and stop conditions)\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "id": "F833NXi05LLr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer, PreTrainedTokenizerFast\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "CONTEXT_LENGTH = 1024\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "\n",
    "    eval_strategy=\"steps\", # Estimation on a validation set\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "\n",
    "    #fp16=torch.cuda.is_available(),\n",
    "    fp16=False, # <-- switch off fp16\n",
    "    bf16=True,\n",
    "    logging_steps=100,\n",
    "\n",
    "    report_to=\"none\",\n",
    ")\n"
   ],
   "metadata": {
    "id": "mQgCGogG6FlU"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "  ]"
   ],
   "metadata": {
    "id": "4dE-gn2M61Wj"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "import torch\n",
    "\n",
    "class GenerationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to visually evaluate model performance during training.\n",
    "    It triggers text generation on specific test prompts at the end of each evaluation phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_prompts, tokenizer, model):\n",
    "        self.test_prompts = test_prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "\n",
    "        self.model.eval() # Switch model to evaluation mode (disables dropout, updates batch norm stats)\n",
    "        device = self.model.device\n",
    "\n",
    "        print(f\"\\n Generating samples at step {state.global_step}\")\n",
    "\n",
    "        for prompt in self.test_prompts:\n",
    "            # Tokenize the prompt and ensure tensors are on the correct device (GPU/CPU)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_length=inputs.shape[1] + 350, # Generate up to 350 new tokens\n",
    "                    do_sample=True,    # Enable sampling strategy (probabilistic generation)\n",
    "                    temperature=0.7,   # Control randomness: 0.7 balances creativity and coherence\n",
    "                    top_k=50,          # Top-K sampling: restricts candidate tokens to the 50 highest probability options\n",
    "                )\n",
    "\n",
    "            # Decode the generated output\n",
    "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "            print(f\"Response: {generated_text}\\n\")\n",
    "\n",
    "        # back to training mode\n",
    "        self.model.train()"
   ],
   "metadata": {
    "id": "OBghVHhq65Ll"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generation_callback = GenerationCallback(questions_rus, tokenizer, model)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[generation_callback],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "85c1bbd982974e5ab2f50a193655aa94",
      "61cc84a5c0ad4b2b848c898030072890",
      "a498a5cbac9b43b1b429daab7ce65155",
      "05f4d7fc6a1241f197bf8d800b90b91f",
      "bbb4d35543704ea9beb60d754a3f76f3",
      "d1b963a84b904ce88d59cf1515c10072",
      "b71413577b2a4351b579a64ac01ea545",
      "a346db746b014eacaacadccca1a9c2bc",
      "840f1261f1974726b53faa3eaf7aa692",
      "58a24bfb0d4544e2a3083c04dabfce5a",
      "3ef7f2af804f48038749d589a590b784",
      "cb6866f8703f4e0f9e44922abf5d2b93",
      "3a3e584f317546d686b56d6c56408c02",
      "5b753e3c406840c0962f745286c4c3ec",
      "21802ff846c34cef8e4f893195f10308",
      "27a61bed56fb407ebcba882e7ed89c21",
      "2f0326d7b22e4d6ca0e65f704c24a57c",
      "dd077a5791154c98935142204a56a251",
      "e87602b38eb74ea693c22407c67e62ad",
      "a1ecf8054c0e47cd85a33873f6ec5454",
      "e1002d0fb81445c5ac2a1ecc98212ccc",
      "3539033526d64aba9f9f329a5806f55b",
      "ba80c1a3057f4e4ba323fb61e232e642",
      "1c0f2c4b5c4a43ef981c8c61d04cab56",
      "66b8906ce1114b4b9138b6c2903e43a1",
      "3fc4e78c89b141c88623c60e71e36e9a",
      "953fca811abc43ed81e93636272a63c0",
      "91d6a11b1e2a48e68b246f85b8a01f10",
      "ef905cbe2ca243c1a6e2001324977e9a",
      "d3a04e8f6c7a4e4ab2ca8f1a952bc5e2",
      "18587700c06c4d1faf74d75c7a23bf0a",
      "2d4fd70e3ef74c4486e60fc7bf73d5d1",
      "cc00c6b4affc40ae8a3b8d6251b4b826",
      "7d8091bcea24461990fb1741957d9324",
      "544bfbe46932446185f8368fb74bfc77",
      "baf1e98ab7ad4e9f9852fe0a0f613533",
      "5d4a78bd8eb440a49af6b27309f075fc",
      "11f38329e09b4e418111768bea566d30",
      "666be3c65eb145d89b59c25436fd9e46",
      "f8f87f003efb490c83e23ffb82e4f4bd",
      "8eb71f57563a493ebfd2a18eff1e4b9b",
      "8e15db83dc0742c8a9cd6f016d9d9cd2",
      "adc98b23cce64c15ad851aee5d0b3f93",
      "e8bf3b9e75714b49a13de9d3431cc0c3",
      "a90af430de004683989d4f670e2932be",
      "231fcec1ccbf4f17910dd9be4d00861f",
      "8295216584ba42f087504be82d0afd03",
      "c02ebd9aae0d41d3832de1bba115326c",
      "71b1fdf0bd234ece962981dd1a00c1b0",
      "ebdef86cc2654ff4a9a9ef94c0653f79",
      "161be790d13040699f6aa0758c12db77",
      "c198b8c0d411424ca91b4c1d1d955987",
      "46a6749c4f3446feaea606d0dfcca5a3",
      "f810c671034d492ea686ed706d97dfdf",
      "2eeea7ca4c11476987485e48c6a1d2c4",
      "d9ec130575c04ee1963f16139351babb",
      "49ffa90221f24a198728ceb7121a6c7a",
      "46ea16130e71464fbd466db69439f40f",
      "abe5e834ccb64dfb863a1048828af2a4",
      "e063e4ef0db94c44a3f1d83e62cf0e6d",
      "a61a2f2731bb4602a722dcf38254ab4a",
      "b2a097ff6d5a4d089ccf0491037a20cc",
      "4109e7b7fc084848aa2aedc4b18a237b",
      "44d0388f35104bfca53e5f21c365cd09",
      "8b2f81b1ec5647b3952d3b83bc579ead",
      "2f90b6b5a8f84068ad9ea1693f72e506"
     ]
    },
    "id": "rQQlLXlv7NGm",
    "outputId": "dfc463f6-4959-427b-da42-f483908eb26e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/49172 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85c1bbd982974e5ab2f50a193655aa94"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/49172 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb6866f8703f4e0f9e44922abf5d2b93"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/49172 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba80c1a3057f4e4ba323fb61e232e642"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2588 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d8091bcea24461990fb1741957d9324"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2588 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a90af430de004683989d4f670e2932be"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2588 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9ec130575c04ee1963f16139351babb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='557' max='4611' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 557/4611 3:32:13 < 25:50:13, 0.04 it/s, Epoch 0.36/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.566200</td>\n",
       "      <td>1.571060</td>\n",
       "      <td>1.668198</td>\n",
       "      <td>838937.000000</td>\n",
       "      <td>0.641912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.493400</td>\n",
       "      <td>1.532754</td>\n",
       "      <td>1.637582</td>\n",
       "      <td>1660367.000000</td>\n",
       "      <td>0.648912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.475700</td>\n",
       "      <td>1.507412</td>\n",
       "      <td>1.601954</td>\n",
       "      <td>2494470.000000</td>\n",
       "      <td>0.653245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.460900</td>\n",
       "      <td>1.489846</td>\n",
       "      <td>1.596124</td>\n",
       "      <td>3336372.000000</td>\n",
       "      <td>0.656478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.459600</td>\n",
       "      <td>1.473911</td>\n",
       "      <td>1.557259</td>\n",
       "      <td>4152267.000000</td>\n",
       "      <td>0.659576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=362) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Generating samples at step 100\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=356) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"сколько планет в нашей солнечной системе?\"\n",
      "Response: сколько планет в нашей солнечной системе? Считайте, что планеты в нашей солнечной системе симметричны.\n",
      "Колонтитул.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=360) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"расскажи стих\"\n",
      "Response: расскажи стихотворение, которое начинается с «тогда».\n",
      "Жить, жить и жить, любить и любить, это наша жизнь.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=358) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"когда собирать крыжовник?\"\n",
      "Response: когда собирать крыжовник? в каких странах он используется?\n",
      "Крыжовник, или крыжовник, — это солдатский и пехотный хлеб. Он может быть разработан в разных странах, но чаще всего он используется в Западной Америке, в Канаде, в Германии и в Англии.\n",
      "\n",
      "Prompt: \"Как быстро выучить новый язык?\"\n",
      "Response: Как быстро выучить новый язык? Дайте мне 10 советов.\n",
      "Делаете это за 5 минут каждый день. Сделайте это, каждый день, и вы сможете быстро научиться новому языку.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=362) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Generating samples at step 200\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=356) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"сколько планет в нашей солнечной системе?\"\n",
      "Response: сколько планет в нашей солнечной системе? Подберите 3 из них.\n",
      "\n",
      "1. Луна. Луна — один из наиболее известных планет нашей солнечной системы, обладающих темпом вращения около 243,9 дней. На ней синий тенденс света, который излучает солнце, проходит, создавая тень и солнечную гамму на ее поверхности.\n",
      "\n",
      "2. Земля. Земля — вторая планета нашей солнечной системы, обладающая темпом вращения около 365,24 дней. Она состоит из материя, содержащей витамины и питательные вещества, связанные с водой и углекислым газом, которые обеспечивают естественный климат и земное тело.\n",
      "\n",
      "3. Марс. Марс — планета нашей солнечной системы, обладающая темпом вращения около 687 дней, также известным как «4318 с». Марс является единственной планетой, в которой растут растения, что позволяет ему иметь более богатый и теплый вегетариансльный климат, чем планеты с большими темпами вращения. Марс также имеет более короткий часовой пояс, что позволяет солнцому свету и теплу быстро обогатиться, что делает его идеальным местом для жизни в течение многих лет.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=360) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"расскажи стих\"\n",
      "Response: расскажи стихотворение\n",
      "\n",
      "Когда я жду, когда я жду,\n",
      "Когда я жду, чтобы было доброе,\n",
      "Когда я жду, я чувствую себя нежным,\n",
      "Когда я жду, я чувствую себя жадным.\n",
      "\n",
      "Когда я жду, в моих сердцах всегда есть,\n",
      "Когда я жду, я чувствую себя нежным,\n",
      "Когда я жду, я чувствую себя жадным,\n",
      "Когда я жду, я чувствую себя нежным.\n",
      "\n",
      "Когда я жду, когда я жду,\n",
      "Когда я жду, я чувствую себя жадным,\n",
      "Когда я жду, в моих сердцах всегда есть,\n",
      "Когда я жду, я чувствую себя нежным.\n",
      "\n",
      "Когда я жду, когда я жду,\n",
      "Когда я жду, я чувствую себя жадным,\n",
      "Когда я жду, в моих сердцах всегда есть,\n",
      "Когда я жду, я чувствую себя нежным.\n",
      "\n",
      "Когда я жду, когда я жду,\n",
      "Когда я жду, я чувствую себя жадным,\n",
      "Когда я жду, в моих сердцах всегда есть,\n",
      "Когда я жду, я чувствую себя нежным.\n",
      "\n",
      "Когда я жду, когда я жду,\n",
      "Когда я жду, я чувствую себя жадным,\n",
      "Когда я жду, в моих сердцах всегда есть,\n",
      "Когда я жду, я чувствую себя нежным.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=358) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"когда собирать крыжовник?\"\n",
      "Response: когда собирать крыжовник? Какие ключевые элементы должны быть включены в этот процесс?\n",
      "\n",
      "Крыжовник — это изучение и анализ процессов, происходящих в организме человека, с целью построения соответствующих принципов и методов лечения болезней. Чтобы собрать крыжовник, необходимы следующие ключевые элементы:\n",
      "\n",
      "1. Определите цель исследования: прежде чем собирать крыжовник, важно определить цель исследования, чтобы определить целевой группу и объективные задачи, которые необходимо решить. Это может быть посредством изучения профиля болезни, отсутствия или развития заболевания, или изучение влияния внешних факторов на болезнь.\n",
      "\n",
      "2. Проведите наблюдение: наблюдение — это основная часть крыжовника, в котором исследования обычно проводятся с использованием датчиков, таких как левитан, симптоматика или химические измерения, чтобы определить структуру болезни и определить ее профилактические и лечебные действия.\n",
      "\n",
      "3. Анализируйте данные: после наблюдения результаты анализируются, чтобы определить структуру болезни и определить ее профилактические и лечебные действия.\n",
      "\n",
      "4. Подготовка к лечению. Далее необходимо подготовить препараты, включая их состав, инструкции и рекомендации. Важно передать все обеоразмерные данные о препарате и его действиям в терапевтическом отношении.\n",
      "\n",
      "5. Составьте программу лечения: программа лечения должна быть адаптирована к народным средствам, обозначенным рецептом, включая состав, инструкции и рекомендации.\n",
      "\n",
      "6. Проведение лечения. Поскольку крыжовник включает в себя определенное количество статических и активных процедур, для разработки программы лечения важно использовать методы, которые совместно сочетаются, чтобы обеспечить эффективное лечение болезни.\n",
      "\n",
      "7. Обновление данных: последующее обновление данных, включая проверку на последствия лечения и определение последствий лечения, необходимы для обновления программы лечения, чтобы обеспечить эффективное лечение болезни.\n",
      "\n",
      "Кроме того, крыжовник также часто включает в себя диагностику поведенческих и психических факторов, такие как социальные и когнитивные проблемы, чтобы убедиться, что лечение было адаптировано к их конкретным ожиданиям. Кроме того, крыжовник также включает в себя интервью с пациентом, чтобы убедиться, что лечение соответствовало ожиданиям и что пациент чувствует себя хорошо после лечения.\n",
      "\n",
      "Prompt: \"Как быстро выучить новый язык?\"\n",
      "Response: Как быстро выучить новый язык? Как быстро выучить новый язык?\n",
      "\n",
      "Эта проблема может быть решена несколькими способами, в том числе:\n",
      "\n",
      "1. Математика. Многие люди воспринимают математику как сложный предмет, который нужно преодолеть, чтобы понять язык. Но умение слушать и умение слежь, это очень важно, и это хорошо сформированное чувство ума может помочь вам быстро читать и слушать. Помните, что вы не можете изучить новые предметы, если вы не слушаете их.\n",
      "\n",
      "2. Переписка. Переписка — одна из самых простых и эффективных способов изучения нового языка. Помимо того, чтобы писать, вы также можете использовать другие методы, такие как читать, слушать и слышать, чтобы лучше освоить язык.\n",
      "\n",
      "3. Скачать и читать. Скачайте словарь и потренируйтесь, читая и слушая. Давайте-ка почитать поэму или рассказ о пресловутом языке, и вы будете в состоянии дать о себе знать.\n",
      "\n",
      "4. Воспринимайте и повторяйте. Иногда просто воспринимать материал несколько раз и повторять его несколько раз, можно помочь вам быстро изучить язык.\n",
      "\n",
      "5. Используйте музыкальную инструменты. Музыкальные инструменты могут помочь вам быстро слушать и понять язык. Включите музыкальный инструмент в свои занятия и используйте его для изучения новых слов в разных предложениях или предложениях.\n",
      "\n",
      "6. Не бойтесь начинать с простого. Если вы начинаете изучать новый язык, не бойтесь начинать с простого примера или предложения. Если вы чувствуете, что у вас есть трудности, используйте перерывы или повторяйте материал, пока вы не достигнете своего целевой уровня.\n",
      "\n",
      "Конечно, для успешного изучения нового языка важно будет продолжать следить за своим уровнем понимания. С помощью этих простых и эффективных методов вы можете быстро изучить язык и быстро поймать язык.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=362) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Generating samples at step 300\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=356) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"сколько планет в нашей солнечной системе?\"\n",
      "Response: сколько планет в нашей солнечной системе? Составьте список из 5 планет в нашей солнечной системе.\n",
      "\n",
      "1. Земля\n",
      "2. Венера\n",
      "3. Юпитер\n",
      "4. Меркурий\n",
      "5. Сатурн\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=360) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"расскажи стих\"\n",
      "Response: расскажи стихотворение о любви\n",
      "\n",
      "В тишине ночи, не дождя,\n",
      "Любовь есть сила волны,\n",
      "Ее ожесточен, громкими огнями,\n",
      "И она наводит на разум.\n",
      "\n",
      "Мы смеем и восхищаемся,\n",
      "Такими ритмичными и мудрыми,\n",
      "Предпоследствиями разлуки,\n",
      "Призывы к соприкосновению.\n",
      "\n",
      "Изнутри, в свете дневного света,\n",
      "Текст, который мы не знаем,\n",
      "Мы смеем и восхищаемся,\n",
      "Такими ритмичными и мудрыми.\n",
      "\n",
      "Мы смеем и восхищаемся,\n",
      "Такими ритмичными и мудрыми,\n",
      "Предпоследствиями разлуки,\n",
      "Призывы к соприкосновению.\n",
      "\n",
      "Но любовь есть и мистика,\n",
      "И она умница, когда мы смеем и восхищаемся,\n",
      "Предпоследствиями разлуки,\n",
      "Призывы к соприкосновению.\n",
      "\n",
      "Мы смеем и восхищаемся,\n",
      "Такими ритмичными и мудрыми,\n",
      "Предпоследствиями разлуки,\n",
      "Призывы к соприкосновению.\n",
      "\n",
      "Мы смеем и восхищаемся,\n",
      "Такими ритмичными и мудрыми,\n",
      "Предпоследствиями разлуки,\n",
      "Призывы к соприкосновению.\n",
      "\n",
      "Одна и та же любовь,\n",
      "Умная и милая,\n",
      "Несмотря на их разницу,\n",
      "Предпоследствия разлуки.\n",
      "\n",
      "Мы смеем и восхищаемся,\n",
      "Такими ритмичными и мудрыми,\n",
      "Предпоследствиями разлуки,\n",
      "Призывы к соприкосновению.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=358) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"когда собирать крыжовник?\"\n",
      "Response: когда собирать крыжовник? Игра\n",
      "Предложите несколько вариантов ответа на этот вопрос.\n",
      "\n",
      "- Охота на крыжовника\n",
      "- Сбор китов\n",
      "- Охота на крыжовника\n",
      "- Расслабляются и спать\n",
      "- Поделиться своими знаниями о крыжовниках\n",
      "- Поделиться своими знаниями о крыжовниках\n",
      "- Погулять и погрузиться в мир крыжовника\n",
      "- Поделиться своими знаниями о крыжовниках\n",
      "- Красота и вдохновение на крыжовника\n",
      "- Расслабляют себя и отдохните\n",
      "- Сделайте закуску с крыжовником\n",
      "- Поделиться своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Сделайте закуску с крыжовником\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Поделитесь своими знаниями о крыжовниках\n",
      "- Погулять и провести время, покормив крыжовника\n",
      "- Поделитесь своими знаниями о крыж\n",
      "\n",
      "Prompt: \"Как быстро выучить новый язык?\"\n",
      "Response: Как быстро выучить новый язык? Есть ли какие-то советы, которые помогут вам с этой задачей?\n",
      "\n",
      "Одна из возможных способов быстро учиться новый язык — использовать онлайн-разработчики для повторения и повторения. Запустите онлайн-разработчик на заданном языке и повторите разговор с помощью него. Это поможет вам усвоить основные пункты и правила, а также создать впечатление о том, как хорошо работают на языке, а также изучите его языки на других языках.\n",
      "\n",
      "Однако важно отметить, что онлайн-разработчики не могут обеспечить чистоту и точность в общении с людьми, поэтому важно использовать людей, чтобы доказать ваши знания и получать обратную связь. Вы также можете обратиться за помощью к онлайн-учебным заведениям и преподавателям в своем регионе.\n",
      "\n",
      "В заключение, в процессе обучения нового языка важно использовать онлайн-разработчик, чтобы усвоить основные принципы и правила, а также создать впечатление о том, как хорошо работают на языке, а также изучить его языки на других языках.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=362) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Generating samples at step 400\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=356) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"сколько планет в нашей солнечной системе?\"\n",
      "Response: сколько планет в нашей солнечной системе? Извините, но у Вас нет такой возможности предоставить ответ на данный вопрос.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=360) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"расскажи стих\"\n",
      "Response: расскажи стихотворение о любви\n",
      "\n",
      "Во солнечном и тепле солнечном и дождливом лесу,\n",
      "Синеющие листья кашляют ветер, и утренний фонтан,\n",
      "Из луны красный, как дождь, и звезды в ночи,\n",
      "Ночь, тишина, ветер, ветер, пропитанный любовью,\n",
      "\n",
      "По мере ветра, и ветер летит,\n",
      "То бывает, что он кажется так же холодный, как небо,\n",
      "Он приходит, когда мы просыпаемся,\n",
      "И он становится настроем, когда мы сидим.\n",
      "\n",
      "Мы смеемся, пока мы смеемся,\n",
      "Воздух не знает, что нас крадет, и мы смеемся,\n",
      "И мы смеемся, пока мы смеемся, и приплюснут,\n",
      "И мы смеемся, пока мы смеемся, и наслаждаемся,\n",
      "\n",
      "Жизнь настолько весна, что мы смеемся, потому что мы смеемся,\n",
      "И мы смеемся, потому что мы смеемся, и мы не можем жить без этого,\n",
      "И мы смеемся, потому что мы смеемся, и мы не можем жить без этого,\n",
      "И мы смеемся, потому что мы смеемся, и мы не можем жить без этого.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=358) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"когда собирать крыжовник?\"\n",
      "Response: когда собирать крыжовник? Пользователь не понимает, что он должен делать, поэтому он опечален.\n",
      "Опишите действия пользователя в следующем сообщении.\n",
      "\n",
      "Игра игрока: «Мне нужно собрать крыжовник!»\n",
      "\n",
      "Prompt: \"Как быстро выучить новый язык?\"\n",
      "Response: Как быстро выучить новый язык? Какие методы эффективны для достижения результата?\n",
      "\n",
      "Методы эффективного обучения новому языку включают множество вариантов, которые могут помочь вам улучшить свои навыки и навыки. Вот некоторые из наиболее эффективных методов:\n",
      "\n",
      "1. Продумайте цель. Просто начните с обучения языку по-прежнему, даже если вы не знаете его. Укажите цель поиска нового языка и настройте на это.\n",
      "\n",
      "2. Соберите данные и разработайте план обучения. Создайте список из примеров и изучайте их с использованием языка. Составьте список языков, которые вы хотите учиться, и определите, какую область или задачу вы хотите продемонстрировать.\n",
      "\n",
      "3. Составьте перерывы. Делайте перерывы, чтобы восстановить свои навыки. Помните, что важно, чтобы вы не делали перерывы слишком часто.\n",
      "\n",
      "4. Следуйте урокам. Студите и повторяйте домашние уроки в течение дня. Не забудьте использовать различные способы обучения, например, прилагательные или переносы текста, чтобы упростить понимание ключевых тем.\n",
      "\n",
      "5. Изучайте язык в режиме реального времени: используйте онлайн-курсы или приложения для запоминания языка. Это можно сделать с помощью текстовых и графических материалов, которые включают в себя отчеты и тесты.\n",
      "\n",
      "6. Изучайте язык в различных средах. Изучайте язык в различных средах, таких как веб-сайты, веб-камеры, музыкальные композиции и музыкальные циклы.\n",
      "\n",
      "7. Внешний контроль. Убедитесь, что вы учитесь в соответствии с собственным уровнем умственного развития. Практикуйте свои навыки, проверяйте свои знания и улучшайте свои навыки.\n",
      "\n",
      "8. Общая практика. Обязательно посещайте обучающие мероприятия и встречи, чтобы общаться со своими коллегами и иметь возможность практиковаться и обмениваться опытом.\n",
      "\n",
      "Таким образом, эффективного обучения нового языка обладает множеством эффективных методов, которые нужно использовать для достижения результата. Пройдя через эти этапы и включая эти методы, вы сможете быстро и эффективно обучиться новому языку.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=362) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Generating samples at step 500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=356) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"сколько планет в нашей солнечной системе?\"\n",
      "Response: сколько планет в нашей солнечной системе? Вместо этого напишите описание и объясните, почему это неправильно.\n",
      "\n",
      "Планеты в нашем солнечном системе — это осязаемые, светопрозрачные и гравитирующие объекты в системе, состоящей из двух планет и нескольких галактик. Несколько планет в нашей системе известны как звездные планеты, а более крупные — планеты, которая были бы обитающими на Земле. По мере увеличения размера планеты она также прокрашивает более широкий диапазон галактик, что означает, что она может обитать на разных галактических сегментах. Когда планета обретает более широкий диапазон галактик, она может стать более густой и более вспышкой света, что приводит к более длинному диапазону света и более низким температурам. Это может привести к более морозным и морякам условиям, чем когда-либо, и может привести к более низким температурам планеты. Однако не менее важным фактором является то, что планеты имеют различные составляющие, такие как магнетический слои, которые могут улавливать и препятствовать распределению света, а также геофизические свойства, такие как тяжелость, масса и давление, которые могут разделять свет и давать планетам свой уникальный вид. С другой стороны, планеты могут быть захватчиками или поковылять, представляя собой отличные образцы того, как планеты могут участвовать в сплывающем солнечном свете.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=360) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"расскажи стих\"\n",
      "Response: расскажи стихотворению о двух мечтах\n",
      "\n",
      "Мечта — это гордость, когда ты чувствуешь себя в своем праве,\n",
      "Мечта — это желание, когда ты чувствуешь себя в себе.\n",
      "Мечта — это навык, когда ты ведешь себя в соответствии со своей волей,\n",
      "В ней нет ничего, чего можно предпринять с другой стороны.\n",
      "\n",
      "Мечта — это энтузиазм, когда ты начинаешь с небольшим и начинаешь изучать каждый день,\n",
      "Мечта — это путь, когда ты начинаешь с небольшим и начинаешь идти дальше.\n",
      "\n",
      "Мечта — это чувство, когда ты чувствуешь себя в своем праве и в своем праве,\n",
      "Когда ты знаешь, что ты можешь делать, когда тебе это нужно.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=358) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: \"когда собирать крыжовник?\"\n",
      "Response: когда собирать крыжовник?.\n",
      "Расскажите, чем может быть связанное с этим предложение.\n",
      "\n",
      "Крыжовник — это небольшая ветвь деревья, которая прикреплена к стволу ее родной деревца. Для собираи крыжовника обычно используется деревянная махина, а также крепление рукоятка. Махина должна быть надежной, толстой и сильной, позволяя крепко придерживать крыжовник. Крыжовника можно собирать в качестве доски для ремонта и ухода за ведро, или для быстрого и эффективного сбора.\n",
      "\n",
      "Prompt: \"Как быстро выучить новый язык?\"\n",
      "Response: Как быстро выучить новый язык? Какие уроки нужны?\n",
      "\n",
      "Вот шаги, которые могут помочь вам быстро и эффективно учиться на новый языке:\n",
      "\n",
      "1. Соберите необходимые ресурсы. Обучение на новый язык требует знаний о языке в целом, его лингвической структуре, навыках и личных предпочтениях. Соберите ресурсы, такие как книги, учебники, онлайн-курсы, интерактивные программы для интервью, видеоклассиконы и другие.\n",
      "\n",
      "2. Определите цель и визуализируйте. Начните с определения ваших ценностей и целей, которые вы хотите достичь, и созаботитесь об этом в течение первого дня обучения. Визуализируйте суть нового языка и остановьтесь на этом.\n",
      "\n",
      "3. Установите четкие цели и перекрестные цели. Начните с четкого определения ваших целей и перекрестных ценностей, и сознательно планируйте, как вы будете выполнить их. Например, если ваше целевое направление — творчество, вы можете создавать свои собственные рассказы, песни или сценарии, а если цель — обучение, вы можете сосредоточиться на понимании и понимании.\n",
      "\n",
      "4. Собирайте свои знания. Собирайте материал и участвуйте в личных занятиях, участвуйте в интервью или участвуйте в онлайн-обучении. Этот метод позволяет вам охватить большую количество информации в один день, и это может помочь вам быстрее и более эффективно учиться.\n",
      "\n",
      "5. Составьте план обучения. Составьте план обучения, который может включать в себя повторение нового языка, выполнение заданий, оценку своих сроках обучения и планы для продолжения обучения в дальнейшем. Это поможет вам организовать свои усилия и направить свое время на нужные области.\n",
      "\n",
      "6. Практикуйте и приспосаблейте. Практикуйте и приспосаблейте свои навыки, используя такие способы, как съемки виртуальной игры, тестирование на разных языках, присваивание и повторение новых языковых упражнений. Это поможет вам освоить язык эффективно и эффективно.\n",
      "\n",
      "7. Помогите другим. Помогите другим, чтобы они поняли, как справляться со стрессом и трудностями, возникшими во время обучения. Важно помочь другим, чтобы вы всегда имели надежную альтернативу в случае необходимости.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Мне не хватило вычислительных мощностей, чтобы провести запланированных 3 эпохи, а разные способы \"облегчения\" работы с gpu после нескольких попыток внедрить не получилось :(\n",
    "# Но даже после сотого шага видно, что ответы относительно осмысленные, по крайней мере на русском языке!"
   ],
   "metadata": {
    "id": "sosJQIV-bdNW"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}